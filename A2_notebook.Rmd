---
title: "Structure Learning"
output:
  pdf_document: default
  html_notebook: default
---

## Structure Learning a Bayesian Network given bike sharing data

Let's start with importing the necessary libraries.

```{r}
#if (!requireNamespace("BiocManager", quietly = TRUE))
#  install.packages("BiocManager")
#BiocManager::install("RBGL")
#install.packages("birankr")
#install.packages("bnlearn")
#intall.packages("SID")
library(birankr)
library(bnlearn)
library(dagitty)
library(ggplot2)
library(ggpubr)
library(SID)
```

Let's now load the data from storage and determine the indices at which each day starts.

```{r}
# use hourly data as source
hourly.data <- read.csv("hour.csv", header=TRUE)
# determine indices of day starts
day.starts <- hourly.data$instant[!duplicated(hourly.data$dteday)]

write(sprintf("Number of days in the dataset: %d", length(day.starts)), stdout())
```
Now let's make the hourly dataset a daily dataset by aggregating over the hour variable.

```{r include=FALSE}
daylength <- function(days)
{
  result <- NULL
  L <- 38.8951  # latitude Washington D.C.
  for (day in days) 
  {
    P = asin(0.39795 * cos(0.2163108 + 2 * atan(0.9671396 * tan(.00860 * (day - 186)))))
    
    D <- 24 - (24 / pi) * 
      acos(
        (sin(0.8333 * pi / 180) + sin(L * pi / 180) * sin(P))
        / 
          (cos(L * pi / 180) * cos(P))
      )
    
    result <- c(result, D)
  }
  return(result)
}


gasprice.per.week <- c(
  3.114, 3.129, 3.141, 3.164, 3.172, 3.165, 3.175, 3.181, 3.209, 3.37, 3.519,
  3.567, 3.562, 3.576, 3.657, 3.757, 3.828, 3.898, 3.981, 4, 3.993, 3.912,
  3.847, 3.805, 3.766, 3.727, 3.672, 3.634, 3.687, 3.738, 3.759, 3.772, 3.738,
  3.67, 3.625, 3.626, 3.666, 3.667, 3.634, 3.567, 3.504, 3.462, 3.498, 3.494,
  3.476, 3.454, 3.454, 3.416, 3.373, 3.338, 3.334, 3.309, 3.313, 3.346, 3.44,
  3.46, 3.494, 3.548, 3.571, 3.627, 3.67, 3.748, 3.78, 3.793, 3.828, 3.882,
  3.924, 3.974, 3.955, 3.93, 3.874, 3.809, 3.764, 3.715, 3.657, 3.603, 3.534,
  3.477, 3.417, 3.384, 3.434, 3.482, 3.564, 3.569, 3.637, 3.725, 3.75, 3.781,
  3.823, 3.86, 3.953, 3.907, 3.893, 3.911, 3.877, 3.787, 3.692, 3.677, 3.66,
  3.657, 3.637, 3.594, 3.545, 3.486, 3.471, 3.5
)
gasprice <- function(days)
{
  week.starts <- c(days[hourly.data[day.starts, "weekday"] == 1], length(days) + 1)
  result <- NULL
  current.monday <- 1
  for (week in 1:length(week.starts))
  {
    next.monday <- week.starts[week]
    result <- c(result, rep(gasprice.per.week[week], next.monday - current.monday))
    current.monday <- next.monday
  }
  return(result)
}

load.data <- function()
{
  data <- data.frame(
    day=seq_along(day.starts),
    season=hourly.data[day.starts, "season"] %% 4 > 1,
    holiday=hourly.data[day.starts, "holiday"] == 1,
    workingday=hourly.data[day.starts, "weekday"] %% 6 != 0,
    weathersit=ordered(1, levels=1:3),
    daylength=daylength(seq_along(day.starts)),
    rallyprotest=seq_along(day.starts) %in% c(
      274, 288, 289, 313, 314, 315, 316, 317, 318, 319, 320, 321,
      322, 323, 324, 325, 326, 327, 376, 416, 449, 575, 673, 687
    ),
    gasprice=gasprice(seq_along(day.starts)),
    hum=-1,
    windspeed=-1,
    temp=-1,
    atemp=-1,
    casual=-1,
    registered=-1
  )
  
  # aggregate over hours to get daily variables
  day.intervals <- c(day.starts, nrow(hourly.data) + 1)
  for (day in data$day)
  {
    day.data <- hourly.data[day.intervals[day]:(day.intervals[day + 1] - 1),]
    
    if (sum(day.data$weathersit == 3 | day.data$weathersit == 4) >= 3)
    {
      data$weathersit[day] <- 3
    } else if (sum(day.data$weathersit == 2 | day.data$weathersit == 3 | day.data$weathersit == 4) >= 3)
    {
      data$weathersit[day] <- 2
    } else
    {
      data$weathersit[day] <- 1
    }
    
    data[day, "hum"] <- mean(day.data$hum)
    data[day, "windspeed"] <- mean(day.data$windspeed)
    data[day, "temp"] <- mean(day.data$temp)
    data[day, "atemp"] <- mean(day.data$atemp)
    
    data[day, "casual"] <- sum(day.data$casual)
    data[day, "registered"] <- sum(day.data$registered)
  }
  return(data)
}
```

```{r}
data <- load.data()
write("Columns in the daily dataset:", stdout())
write(colnames(data), stdout())
```
Now that we have the data, let's make sure that the types are correct.

```{r}
# scale all variables except the ordinal weather situation to z-scores
numeric.data <- data.frame(lapply(subset(data, select=-weathersit), as.numeric))
scaled.data <- data.frame(scale(numeric.data))

# put ordinal weather situation back in dataframes
data <- cbind(numeric.data, weathersit=data$weathersit)
scaled.data <- cbind(scaled.data, weathersit=data$weathersit)
scaled.data$atemp[595] <- -scaled.data$atemp[595]  # reverse outlier
factor.data <- cbind(
  subset(scaled.data, select=-c(season, holiday, workingday, rallyprotest)),
  season=as.factor(data$season),
  holiday=as.factor(data$holiday),
  workingday=as.factor(data$workingday),
  rallyprotest=as.factor(data$rallyprotest)
)
n.factor.data <- cbind(
  subset(data, select=-c(season, holiday, workingday, rallyprotest)),
  season=as.factor(data$season),
  holiday=as.factor(data$holiday),
  workingday=as.factor(data$workingday),
  rallyprotest=as.factor(data$rallyprotest)
)
```

```{r include=FALSE}
non.numeric.cols = c("rallyprotest", "holiday", "season", "workingday", "weathersit")
remove.illegal.arcs <- function(g)
{
  # remove illegal arcs from given graph
  illegal.arcs <- arcs(g)[,"to"] %in% non.numeric.cols
  arcs(g) <- arcs(g)[!illegal.arcs,]
  return(g)
}

run.networks <- function(n.iters, pred, alg, ...)
{
  graphs <<- append(graphs, list(vector("list", n.iters)))
  
  if (pred)  # Hill Climbing algo
  {
    years.list <<- append(years.list, list(vector("list", n.iters)))
    for (i in 1:n.iters)
    {
      # generate random DAG with no illegal arcs
      rand.graph <- random.graph(colnames(data), method="melancon")
      rand.graph <- remove.illegal.arcs(rand.graph)
      
      # learn structure starting from generated DAG given the data
      g <- alg(..., start=rand.graph, restart=5, perturb=10, max.iter=1000)
      save.network(g, i, pred)
    }
    
  } else {  # PC algo
    for (i in 1:n.iters)
    {
      g <- alg(...)
      save.network(g, i, pred)
    }
  }
}

save.network <- function(g, i, pred)
{
  graphs[[length(graphs)]][[i]] <<- g
  
  if (pred)
  {
    g <- remove.illegal.arcs(g)
    
    # predict with SEM using bnlearn package
    fit <- bn.fit(g, n.factor.data)  # use non-scaled factor version of data for prediction
    years <- subset(n.factor.data, select=-c(casual, registered))
    years$casual <- predict(fit, node="casual", data=years, method="bayes-lw")
    years$registered <- predict(fit, node="registered", data=years, method="bayes-lw")
    years.list[[length(graphs)]][[i]] <<- years
  }
}

plot.preds <- function(i, j, title)
{
  plot(ggarrange(
    ggplot(data) +
      geom_line(aes(x=day, y=casual)) +
      geom_line(aes(x=day, y=casual + registered)) +
      geom_ribbon(aes(x=day, ymin=0, ymax=casual), fill="red", alpha=0.5) +
      geom_ribbon(aes(x=day, ymin=casual, ymax=casual + registered), fill="blue", alpha=0.5) +
      ylim(-600, max(data$casual + data$registered)) +
      xlab("days") + ylab("True total number of users"),
    ggplot(years.list[[i]][[j]]) +
      geom_line(aes(x=day, y=casual)) +
      geom_line(aes(x=day, y=casual + registered)) +
      geom_ribbon(aes(x=day, ymin=0, ymax=casual), fill="red", alpha=0.5) +
      geom_ribbon(aes(x=day, ymin=casual, ymax=casual + registered), fill="blue", alpha=0.5) +
      ylim(-600, max(data$casual + data$registered)) +
      ggtitle(title) + xlab("days") + ylab("Predicted total number of users"),
    nrow=2
  ))
}

graphsMSEs <- function(titles)
{
  ntitles <- length(titles)
  metrics <- data.frame(matrix(
    c(rep(0, len=4*ntitles), rep(Inf, len=ntitles)),
    byrow=TRUE,
    nrow=5,
    ncol=ntitles,
    dimnames=list(c("mse.cas", "mse.reg", "mse.tot", "bestMSEidx", "0"), titles)
  ))
  
  for (i in 1:ntitles)
  {
    for (j in 1:length(years.list[[i]]))
    {
      graph <- graphs[[i]][[j]]
      yrs <- years.list[[i]][[j]]
      mses <- c(
        mean((data$casual - yrs$casual)^2),
        mean((data$registered - yrs$registered)^2),
        mean(((data$casual + data$registered) - (yrs$casual + yrs$registered))^2)
      )
      metrics[1:3,i] <- metrics[1:3,i] + (mses - metrics[1:3,i]) / j
      
      if (mses[3] < metrics[5,i])
      {
        metrics[4,i] <- j
        metrics[5,i] <- mses[3]
      }
    }
  }
  
  return(metrics[1:4,])
}

graphsMetrics <- function(titles)
{
  ntitles <- length(titles)
  metrics <- data.frame(matrix(
    c(rep(0, len=5*ntitles), rep(Inf, len=2*ntitles)),
    byrow=TRUE,
    nrow=7,
    ncol=ntitles,
    dimnames=list(c("arcs", "SID", "SHD", "bestSIDidx", "bestSHDidx", "0", "1"), titles)
  ))
  
  true.graph <- graphs[[1]][[1]]
  for (i in 1:ntitles)
  {
    for (j in 1:length(graphs[[i]]))
    {
      learned.graph <- graphs[[i]][[j]]
      arcs.sid.shd <- c(
        nrow(arcs(learned.graph)),
        structIntervDist(amat(true.graph), amat(learned.graph))$sid,
        hammingDist(amat(true.graph), amat(learned.graph))
      )
      metrics[1:3,i] <- metrics[1:3,i] + (arcs.sid.shd - metrics[1:3,i]) / j
      
      if (arcs.sid.shd[2] < metrics[6,i])
      {
        metrics[4,i] <- j
        metrics[6,i] <- arcs.sid.shd[2]
      }
      if (arcs.sid.shd[3] < metrics[7,i])
      {
        metrics[5,i] <- j
        metrics[7,i] <- arcs.sid.shd[3]
      }
    }
  }
  
  MECs <- graphsMECs(titles)
  
  return(rbind(metrics[1:5,], MECs))
}

graphsPageRanks <- function(titles)
{
  ntitles <- length(titles)
  pageRanks <- data.frame(matrix(
    rep(0, len=ncol(data)*ntitles),
    nrow=ncol(data),
    ncol=ntitles,
    dimnames=list(colnames(data), titles)
  ))
  
  for (i in 1:ntitles)
  {
    for (j in 1:length(graphs[[i]]))
    {
      graph <- graphs[[i]][[j]]
      gPageRanks <- pagerank(data.frame(arcs(graph)), is_bipartite=FALSE)
      perm <- order(match(colnames(data), gPageRanks$from))[1:nrow(gPageRanks)]
      
      pageRanks[perm,i] <- pageRanks[perm,i] + (gPageRanks$rank - pageRanks[perm,i]) / j
    }
  }
  
  return(pageRanks)
}

compareArcs <- function(arcs1, arcs2)
{
  n.arcs <- nrow(arcs1)
  n <- 0
  for (i in 1:n.arcs)
  {
    if (arcs1[,"from"][i] != arcs2[,"from"][i] | arcs1[,"to"][i] != arcs2[,"to"][i])
    {
      break
    }
    n <- n + 1
  }
  
  return(n == n.arcs)
}

graphsMECs <- function(titles)
{
  ntitles <- length(titles)
  metrics <- data.frame(matrix(
    rep(0, len=ntitles),
    nrow=1,
    ncol=ntitles,
    dimnames=list("MECs", titles)
  ))
  
  for (i in 1:ntitles)
  {
    mecs.arcs <- list()
    for (j in 1:length(graphs[[i]]))
    {
      new.mec <- TRUE
      g <- remove.illegal.arcs(graphs[[i]][[j]])
      cpdag.arcs <- arcs(cpdag(g))
      for (mec.arcs in mecs.arcs)
      {
        if (nrow(mec.arcs) != nrow(cpdag.arcs))
        {
          next
        }
        if (compareArcs(mec.arcs, cpdag.arcs))
        {
          new.mec <- FALSE
          break
        }
      }
      
      if (new.mec)
      {
        mecs.arcs <- append(mecs.arcs, list(cpdag.arcs))
      }
    }
    metrics[1,i] <- length(mecs.arcs)
  }
  
  return(metrics)
}

# initialize lists that will hold graphs, respectively predictions
graphs <- list(vector("list", 1))
years.list <- list(vector("list", 1))
```

### Manual
First, we initialize the manually constructed network to compare the
automatically constructed networks to.

```{r}
# full Bayesian network is specified below using dagitty package
manual.g <- dagitty(paste('dag {',
    ####################################
    #            Variables             #
    ####################################
    # weather variables
    'weathersit [pos="-0.402,-0.329"]
    hum [pos="-0.412,-0.329"]
    windspeed [pos="-0.412,-0.337"]
    temp [pos="-0.421,-0.324"]
    atemp [pos="-0.421,-0.331"]',
    
    # day variables
    'season [pos="-0.428,-0.312"]
    day [pos="-0.422,-0.297"]
    workingday [pos="-0.402,-0.319"]
    holiday [pos="-0.403,-0.301"]',
    
    # extra variables
    'daylength [pos="-0.422,-0.316"]
    gasprice [pos="-0.422,-0.308"]
    rallyprotest [pos="-0.402,-0.308"]',
    
    # outcome variables
    'casual [pos="-0.412,-0.304"]
    registered [pos="-0.412,-0.321"]',
    
    ####################################
    #            Connections           #
    ####################################
    # weather relations
    'season -> daylength -> temp -> atemp
    weathersit -> {windspeed -> hum} -> atemp',
    
    # gas price relations
    '{season day} -> gasprice',
    
    # outcome relations
    '{day atemp workingday weathersit gasprice daylength holiday rallyprotest} -> casual
    {day atemp workingday weathersit gasprice daylength} -> registered',
'}'))
manual.g <- model2network(toString(manual.g, "bnlearn"))

# save network to lists and plot the network
save.network(manual.g, 1, pred=TRUE)
plot(manual.g, main="Manually constructed network")
```

### Hill Climbing
We are finally ready to start trying out structure learning algorithms.
First, we will try the hill climbing algorithm which learns structure based on a score.

```{r}
# reset lists and set scores that will be used
graphs <- graphs[1]
years.list <- years.list[1]
scores <- c("aic-cg", "bic-cg", "loglik-cg")  # "pred-loglik-cg"

# initialize data for pred-loglik-cg scoring function
set.seed(2020)
prop <- 0.8
cut <- nrow(data) %/% (1 / prop) + 1
factor.data100 <- factor.data[sample(nrow(data)),]
factor.data80 <- factor.data100[1:cut,]
factor.data20 <- factor.data100[(cut + 1):nrow(data),]

# run all algorithm variants
n.iters = 100
run.networks(n.iters, TRUE, hc, factor.data, score=scores[1])
run.networks(n.iters, TRUE, hc, factor.data, score=scores[2])
run.networks(n.iters, TRUE, hc, factor.data, score=scores[3])
#run.networks(n.iters, TRUE, hc, factor.data80, score="pred-loglik-cg", newdata=factor.data20)

# compute and show the metrics
titles <- c("manual", scores)
MSEs <- graphsMSEs(titles); print(MSEs)
metrics <- graphsMetrics(titles); print(metrics)
pageRanks <- graphsPageRanks(titles); print(pageRanks)
```

```{r}
for (i in 1:length(titles))
{
  bestIdx <- metrics["bestSIDidx",i]
  plot(graphs[[i]][[bestIdx]], main=sprintf("Best %s network", titles[i]))
}
```
```{r}
for (i in 1:length(titles))
{
  bestIdx <- MSEs["bestMSEidx",i]
  title <- sprintf("Best %s predictions MSE=%.0f", titles[i], MSEs["mse.tot", i])
  plot.preds(i, bestIdx, title)
}
```

### PC
Next, we will consider a variant of the PC algorithm.

```{r}
graphs <- graphs[1]
years.list <- years.list[1]
alphas = c(0.005, 0.01)

# run all algorithm variants
n.iters = 10
run.networks(n.iters, FALSE, pc.stable, factor.data, alpha=alphas[1])
run.networks(n.iters, FALSE, pc.stable, factor.data, alpha=alphas[2])

# compute and show the metrics
titles = c("manual", paste0("alpha=", alphas))
metrics <- graphsMetrics(titles); print(metrics)
pageRanks <- graphsPageRanks(titles); print(pageRanks)
```

```{r}
for (i in 1:length(titles))
{
  bestIdx <- metrics["bestSIDidx",i]
  plot(graphs[[i]][[bestIdx]], main=sprintf("Best %s network", titles[i]))
}
```
