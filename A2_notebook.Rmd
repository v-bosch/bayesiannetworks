# Structure Learning a Bayesian Network given bike sharing data

Let's start with importing the necessary libraries.

```{r}
# run the commented lines in the console to install the necessary packages
#if (!requireNamespace("BiocManager", quietly=TRUE))
#  install.packages("BiocManager")
#BiocManager::install("RBGL")
#install.packages(c("birankr", "SID", "sna", "bnlearn"))

# load the packages in the current session
library(birankr)
library(dagitty)
library(SID)
library(sna)
library(bnlearn)
```

Let's now load the data from storage and determine the indices at which each day starts.

```{r}
# use hourly data as source
hourly.data <- read.csv("hour.csv", header=TRUE)
# determine indices of day starts
day.starts <- hourly.data$instant[!duplicated(hourly.data$dteday)]

write(sprintf("Number of days in the dataset: %d", length(day.starts)), stdout())
```

```{r include=FALSE}
daylength <- function(days) {
  result <- NULL
  L <- 38.8951  # latitude Washington D.C.
  for (day in days) {
    P = asin(0.39795 * cos(0.2163108 + 2 * atan(0.9671396 * tan(.00860 * (day - 186)))))
    
    D <- 24 - (24 / pi) * 
      acos(
        (sin(0.8333 * pi / 180) + sin(L * pi / 180) * sin(P))
        / 
          (cos(L * pi / 180) * cos(P))
      )
    
    result <- c(result, D)
  }
  
  return(result)
}


gasprice.per.week <- c(
  3.114, 3.129, 3.141, 3.164, 3.172, 3.165, 3.175, 3.181, 3.209, 3.37, 3.519,
  3.567, 3.562, 3.576, 3.657, 3.757, 3.828, 3.898, 3.981, 4, 3.993, 3.912,
  3.847, 3.805, 3.766, 3.727, 3.672, 3.634, 3.687, 3.738, 3.759, 3.772, 3.738,
  3.67, 3.625, 3.626, 3.666, 3.667, 3.634, 3.567, 3.504, 3.462, 3.498, 3.494,
  3.476, 3.454, 3.454, 3.416, 3.373, 3.338, 3.334, 3.309, 3.313, 3.346, 3.44,
  3.46, 3.494, 3.548, 3.571, 3.627, 3.67, 3.748, 3.78, 3.793, 3.828, 3.882,
  3.924, 3.974, 3.955, 3.93, 3.874, 3.809, 3.764, 3.715, 3.657, 3.603, 3.534,
  3.477, 3.417, 3.384, 3.434, 3.482, 3.564, 3.569, 3.637, 3.725, 3.75, 3.781,
  3.823, 3.86, 3.953, 3.907, 3.893, 3.911, 3.877, 3.787, 3.692, 3.677, 3.66,
  3.657, 3.637, 3.594, 3.545, 3.486, 3.471, 3.5
)
gasprice <- function(days) {
  week.starts <- c(days[hourly.data[day.starts, "weekday"] == 1], length(days) + 1)
  result <- NULL
  current.monday <- 1
  for (week in 1:length(week.starts)) {
    next.monday <- week.starts[week]
    result <- c(result, rep(gasprice.per.week[week], next.monday - current.monday))
    current.monday <- next.monday
  }
  
  return(result)
}

load.data <- function() {
  data <- data.frame(
    day=seq_along(day.starts),
    season=hourly.data[day.starts, "season"] %% 4 > 1,
    holiday=hourly.data[day.starts, "holiday"] == 1,
    workingday=hourly.data[day.starts, "weekday"] %% 6 != 0,
    weathersit=ordered(1, levels=1:3),
    daylength=daylength(seq_along(day.starts)),
    rallyprotest=seq_along(day.starts) %in% c(
      274, 288, 289, 313, 314, 315, 316, 317, 318, 319, 320, 321,
      322, 323, 324, 325, 326, 327, 376, 416, 449, 575, 673, 687
    ),
    gasprice=gasprice(seq_along(day.starts)),
    hum=-1,
    windspeed=-1,
    temp=-1,
    atemp=-1,
    casual=-1,
    registered=-1
  )
  
  # aggregate over hours to get daily variables
  day.intervals <- c(day.starts, nrow(hourly.data) + 1)
  for (day in data$day) {
    day.data <- hourly.data[day.intervals[day]:(day.intervals[day + 1] - 1),]
    
    if (sum(day.data$weathersit == 3 | day.data$weathersit == 4) >= 3) {
      data$weathersit[day] <- 3
    } else if (sum(day.data$weathersit == 2 | day.data$weathersit == 3 | day.data$weathersit == 4) >= 3) {
      data$weathersit[day] <- 2
    } else {
      data$weathersit[day] <- 1
    }
    
    data[day, "hum"] <- mean(day.data$hum)
    data[day, "windspeed"] <- mean(day.data$windspeed)
    data[day, "temp"] <- mean(day.data$temp)
    data[day, "atemp"] <- mean(day.data$atemp)
    
    data[day, "casual"] <- sum(day.data$casual)
    data[day, "registered"] <- sum(day.data$registered)
  }
  
  return(data)
}
```

Now let's make the hourly dataset a daily dataset by aggregating over the hour variable.

```{r}
data <- load.data()
```

Now that we have the data, let's make sure that the types are correct.

```{r}
# scale all variables except the ordinal weather situation to z-scores
numeric.data <- data.frame(lapply(subset(data, select=-weathersit), as.numeric))
scaled.data <- data.frame(scale(numeric.data))

# put ordinal weather situation back in dataframes
data <- cbind(numeric.data, weathersit=data$weathersit)
scaled.data <- cbind(scaled.data, weathersit=data$weathersit)

# factorize the binary season, holiday, workingday, and rallyprotest variables
factor.data <- cbind(
  subset(scaled.data, select=-c(season, holiday, workingday, rallyprotest)),
  season=as.factor(data$season),
  holiday=as.factor(data$holiday),
  workingday=as.factor(data$workingday),
  rallyprotest=as.factor(data$rallyprotest)
)

# write variables and variable types to notebook
write("Variables in the daily dataset:", stdout())
for (var in colnames(factor.data)) {
  write(cat(var, ": ", class(factor.data[,var])), stdout())
}
```

```{r include=FALSE}
non.numeric.cols = c("rallyprotest", "holiday", "season", "workingday", "weathersit")
remove.illegal.arcs <- function(g) {
  # remove arcs from numeric to non-numeric variables
  from.non.numeric.arcs <- arcs(g)[,"from"] %in% non.numeric.cols
  to.numeric.arcs <- !arcs(g)[,"to"] %in% non.numeric.cols
  
  arcs(g) <- arcs(g)[to.numeric.arcs | from.non.numeric.arcs,]
  return(g)
}

run.networks <- function(n.iters, variant, alg, ...)
{
  graphs <<- append(graphs, list(vector("list", n.iters)))
  
  for (i in 1:n.iters) {
    if (variant == 1) {  # Hill Climbing starting from random DAG
      # generate random DAG with no illegal arcs
      rand.graph <- random.graph(colnames(data), method="melancon")
      rand.graph <- remove.illegal.arcs(rand.graph)
      
      # learn structure starting from generated DAG given the data
      g <- alg(..., start=rand.graph, restart=5, perturb=10, max.iter=10000)
    } else {  # PC algorithm or Hill Climbing starting from empty DAG
      g <- alg(...)
    }
    
    graphs[[length(graphs)]][[i]] <<- cpdag(remove.illegal.arcs(g))
  }
}

graphsMetrics <- function(titles) {
  arcs.dists <- graphsArcsDists(titles)
  MECs <- graphsMECs(titles)
  centralities <- graphsCentralities(titles)
  reversals <- graphsReversals(titles)
  
  confMatrices <- graphsConfMatrices(titles)
  micro.precisions <- confMatrices[1,] / (confMatrices[1,] + confMatrices[2,])
  micro.recalls <- confMatrices[1,] / (confMatrices[1,] + confMatrices[3,])
  micro.f1s <- 2 * micro.precisions * micro.recalls / (micro.precisions + micro.recalls)
  
  confMatrices <- sweep(confMatrices, 2, unlist(lapply(graphs, length)), "/")
  DDMs <- (confMatrices[1,] + reversals[1,] / 2 - confMatrices[2,] - confMatrices[3,]) / 24
  BSFs <- ((confMatrices[1,] - confMatrices[3,]) / 24 + (confMatrices[4,] - confMatrices[2,]) / 67) / 2
  
  return(rbind(
    arcs.dists, MECs, centralities, confMatrices, Precision=micro.precisions,
    Recall=micro.recalls, F1=micro.f1s, DDM=DDMs, BSF=BSFs
  ))
}

graphsArcsDists <- function(titles) {
  # computes mean number of arcs and mean SID and SHD metrics compared to manual network
  ntitles <- length(titles)
  metrics <- data.frame(matrix(
    c(rep(0, len=5*ntitles), rep(Inf, len=2*ntitles)),
    byrow=TRUE,
    nrow=7,
    ncol=ntitles,
    dimnames=list(c("arcs", "SID", "SHD", "bestSIDidx", "bestSHDidx", "0", "1"), titles)
  ))
  
  true.graph <- graphs[[1]][[1]]
  for (i in 1:ntitles) {
    for (j in 1:length(graphs[[i]])) {
      learned.graph <- graphs[[i]][[j]]
      arcs.sid.shd <- c(
        narcs(learned.graph),
        structIntervDist(amat(true.graph), amat(learned.graph))$sid,
        hammingDist(amat(true.graph), amat(learned.graph))
      )
      metrics[1:3,i] <- metrics[1:3,i] + (arcs.sid.shd - metrics[1:3,i]) / j
      
      if (arcs.sid.shd[2] < metrics[6,i]) {
        metrics[4,i] <- j
        metrics[6,i] <- arcs.sid.shd[2]
      }
      
      if (arcs.sid.shd[3] < metrics[7,i]) {
        metrics[5,i] <- j
        metrics[7,i] <- arcs.sid.shd[3]
      }
    }
  }
  
  return(metrics[1:5,])
}

compareArcs <- function(arcs1, arcs2) {
  # determines whether arc sets are exactly the same
  n.arcs <- nrow(arcs1)
  n <- 0
  for (i in 1:n.arcs) {
    if (arcs1[,"from"][i] != arcs2[,"from"][i] | arcs1[,"to"][i] != arcs2[,"to"][i]) {
      break
    }
    n <- n + 1
  }
  
  return(n == n.arcs)
}

graphsMECs <- function(titles) {
  # determines number of MECs in learned networks of each algorithm variant
  ntitles <- length(titles)
  metrics <- data.frame(matrix(
    rep(0, len=ntitles),
    nrow=1,
    ncol=ntitles,
    dimnames=list("MECs", titles)
  ))
  
  for (i in 1:ntitles) {
    mecs.arcs <- list()
    for (j in 1:length(graphs[[i]])) {
      new.mec <- TRUE
      cpdag.arcs <- arcs(graphs[[i]][[j]])
      for (mec.arcs in mecs.arcs) {
        if (nrow(mec.arcs) != nrow(cpdag.arcs)) {
          next
        }
        if (compareArcs(mec.arcs, cpdag.arcs)) {
          new.mec <- FALSE
          break
        }
      }
      
      if (new.mec) {
        mecs.arcs <- append(mecs.arcs, list(cpdag.arcs))
      }
    }
    metrics[1,i] <- length(mecs.arcs)
  }
  
  return(metrics)
}

graphsCentralities <- function(titles) {
  # computes mean degree and betweenness graph centralities
  ntitles <- length(titles)
  metrics <- data.frame(matrix(
    rep(0, len=2*ntitles),
    nrow=2,
    ncol=ntitles,
    dimnames=list(c("DegreeC", "BetweennessC"), titles)
  ))
  
  for (i in 1:ntitles) {
    for (j in 1:length(graphs[[i]])) {
      graph <- graphs[[i]][[j]]
      max.deg <- 0
      H <- nnodes(graph)^2 - 3*nnodes(graph) + 2
      for (k in 1:nnodes(graph)) {
        deg <- degree(graph, nodes(graph)[k])
        metrics[1,i] <- metrics[1,i] - deg / H
        max.deg <- max(max.deg, deg)
      }
      metrics[1,i] <- metrics[1,i] + nnodes(graph) * max.deg / H
      
      betweennesses <- betweenness(amat(graph))
      metrics[2,i] <- metrics[2,i] - sum(betweennesses) / H
      metrics[2,i] <- metrics[2,i] + nnodes(graph) * max(betweennesses) / H
    }
    metrics[1:2,i] <- metrics[1:2,i] / length(graphs[[i]])
  }
  
  return(metrics)
}

graphsReversals <- function(titles) {
  # computes mean number of reversed edges in the learned networks
  ntitles <- length(titles)
  metrics <- data.frame(matrix(
    rep(0, len=ntitles),
    nrow=1,
    ncol=ntitles,
    dimnames=list("r", titles)
  ))
  
  true.arcs <- arcs(graphs[[1]][[1]])
  K <- nrow(true.arcs)
  for (i in 1:ntitles) {
    for (j in 1:length(graphs[[i]])) {
      learned.arcs <- arcs(graphs[[i]][[j]])
      for (k in 1:K) {
        from <- true.arcs[k,"from"]
        to <- true.arcs[k,"to"]
        froms <- learned.arcs[learned.arcs[,"to"] == from,"from"]
        tos <- learned.arcs[learned.arcs[,"to"] == to,"from"]
        metrics[1,i] <- metrics[1,i] + (to %in% froms) * !(from %in% tos)
      }
    }
    metrics[1,i] <- metrics[1,i] / length(graphs[[i]])
  }
  
  return(metrics)
}

graphsConfMatrices <- function(titles) {
  # computes mean true and false positives and true and false negatives of 
  # learned CPDAGs compared to manual network
  ntitles <- length(titles)
  metrics <- data.frame(matrix(
    rep(0, len=4*ntitles),
    nrow=4,
    ncol=ntitles,
    dimnames=list(c("TP", "FP", "FN", "TN"), titles)
  ))
  
  true.arcs <- arcs(graphs[[1]][[1]])
  K <- nrow(true.arcs)
  for (i in 1:ntitles) {
    for (j in 1:length(graphs[[i]])) {
      correct.arcs <- 0
      learned.arcs <- arcs(graphs[[i]][[j]])
      for (k in 1:K) {
        from <- true.arcs[k,"from"]
        tos <- learned.arcs[learned.arcs[,"from"] == from,"to"]
        arc.in <- true.arcs[k,"to"] %in% tos
        metrics[1,i] <- metrics[1,i] + arc.in
        correct.arcs <- correct.arcs + arc.in
      }
      metrics[2,i] <- metrics[2,i] + narcs(graphs[[i]][[j]]) - correct.arcs
      metrics[3,i] <- metrics[3,i] + K - correct.arcs
      metrics[4,i] <- metrics[4,i] + 91 + correct.arcs - narcs(graphs[[i]][[j]]) - K
    }
  }
  
  return(metrics)
}

graphsPageRanks <- function(titles) {
  # computes mean PageRanks of each variable in the learned networks
  ntitles <- length(titles)
  pageRanks <- data.frame(matrix(
    rep(0, len=ncol(data)*ntitles),
    nrow=ncol(data),
    ncol=ntitles,
    dimnames=list(colnames(data), titles)
  ))
  
  for (i in 1:ntitles) {
    for (j in 1:length(graphs[[i]])) {
      graph <- graphs[[i]][[j]]
      gPageRanks <- pagerank(data.frame(arcs(graph)), is_bipartite=FALSE)
      perm <- order(match(colnames(data), gPageRanks$from))[1:nrow(gPageRanks)]
      
      pageRanks[perm,i] <- pageRanks[perm,i] + (gPageRanks$rank - pageRanks[perm,i]) / j
    }
  }
  
  return(pageRanks)
}

# initialize list that will hold all networks, starting with manual network
graphs <- list(vector("list", 1))
```

### Manual
First, we initialize the manually constructed network to compare the
automatically constructed networks to.

```{r}
# full Bayesian network is specified below using dagitty package
manual.g <- dagitty(paste('dag {',
    ####################################
    #            Variables             #
    ####################################
    # weather variables
    'weathersit [pos="-0.402,-0.329"]
    hum [pos="-0.412,-0.329"]
    windspeed [pos="-0.412,-0.337"]
    temp [pos="-0.421,-0.324"]
    atemp [pos="-0.421,-0.331"]',
    
    # day variables
    'season [pos="-0.428,-0.312"]
    day [pos="-0.422,-0.297"]
    workingday [pos="-0.402,-0.319"]
    holiday [pos="-0.403,-0.301"]',
    
    # extra variables
    'daylength [pos="-0.422,-0.316"]
    gasprice [pos="-0.422,-0.308"]
    rallyprotest [pos="-0.402,-0.308"]',
    
    # outcome variables
    'casual [pos="-0.412,-0.304"]
    registered [pos="-0.412,-0.321"]',
    
    ####################################
    #            Connections           #
    ####################################
    # weather relations
    'season -> daylength -> temp -> atemp
    weathersit -> {windspeed -> hum} -> atemp',
    
    # gas price relations
    '{season day} -> gasprice',
    
    # outcome relations
    '{day atemp workingday weathersit gasprice daylength holiday rallyprotest} -> casual
    {day atemp workingday weathersit gasprice daylength} -> registered',
'}'))
manual.g <- model2network(toString(manual.g, "bnlearn"))

# save network to list and plot the network
graphs[[1]][[1]] <- manual.g
plot(manual.g, main="Manually constructed network")
```

### Hill Climbing
We are finally ready to start trying out structure learning algorithms.
First, we will try the hill climbing algorithm which learns structure based on a score.

```{r}
# reset list and set score functions that will be used
graphs <- graphs[1]
scores <- c("bic-cg", "pred-loglik-cg", "aic-cg", "loglik-cg")

# initialize data for pred-loglik-cg scoring function
set.seed(2020)
prop <- 0.8
cut <- nrow(data) %/% (1 / prop) + 1
factor.data100 <- factor.data[sample(nrow(data)),]
factor.data80 <- factor.data100[1:cut,]
factor.data20 <- factor.data100[(cut + 1):nrow(data),]

# run all algorithm variants
n.iters = 1  # number of networks learned per algorithm variant
random <- FALSE  # whether to use a random or empty initial DAG
run.networks(n.iters, 2 - random, hc, factor.data, score=scores[1])
run.networks(n.iters, 2 - random, hc, factor.data80, score=scores[2], newdata=factor.data20)
run.networks(n.iters, 2 - random, hc, factor.data, score=scores[3])
run.networks(n.iters, 2 - random, hc, factor.data, score=scores[4])
```
We will evaluate the learned networks based on metrics and the PageRanks of
each node in the learned networks.

```{r}
# compute and show the metrics and PageRanks
titles <- c("manual", scores)
metrics <- graphsMetrics(titles); print(metrics)
pageRanks <- graphsPageRanks(titles); print(pageRanks)
```

Finally, we will inspect the learned networks themselves. The learned network
with the smallest SID metric of each algorithm variant is shown.

```{r}
for (i in 1:length(titles)) {
  bestIdx <- metrics["bestSIDidx",i]
  plot(graphs[[i]][[bestIdx]], main=sprintf("Best %s network", titles[i]))
}
```

### PC
Next, we will consider an implementation of the PC algorithm.

```{r}
# reset list and set alpha thresholds that will be used
graphs <- graphs[1]
alphas = c(0.0001, 0.001, 0.01, 0.05, 0.1)

# run all algorithm variants
n.iters = 1  # number of networks learned per algorithm variant
for (alpha in alphas) {
  run.networks(n.iters, 0, pc.stable, factor.data, alpha=alpha, test="mi-cg")
}
```

We will again inspect the metrics and PageRanks of the learned networks.

``` {r}
# compute and show the metrics and PageRanks
titles = c("manual", paste0("alpha=", alphas))
metrics <- graphsMetrics(titles); print(metrics)
pageRanks <- graphsPageRanks(titles); print(pageRanks)
```

Finally, we will inspect the learned networks of the PC algorithm.

```{r}
for (i in 1:length(titles)) {
  bestIdx <- metrics["bestSIDidx",i]
  plot(graphs[[i]][[bestIdx]], main=sprintf("Best %s network", titles[i]))
}
```
